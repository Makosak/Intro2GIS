df_grouped_avg = df_avg %>% group_by(group) %>% summarise_all(funs(mean))
df_grouped_bin = df_bin %>% group_by(group) %>% summarise_all(funs(mean))
library(reshape2)
counts = melt(df_grouped, id.vars='group')
avgs = melt(df_grouped_avg, id.vars='group')
binary = melt(df_grouped_bin, id.vars='group')
library(ggplot2)
ggplot(counts,aes(x=variable, y=value, fill=factor(group)))+geom_bar(stat='identity', position='dodge')+xlab('feature')+ylab('Instance per message') + theme(text = element_text(size=10),
axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('counts per message')
ggplot(avgs,aes(x=variable, y=value, fill=factor(group)))+geom_bar(stat='identity', position='dodge')+xlab('feature')+ylab('Average per message') + theme(text = element_text(size=10),
axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('average per message')
ggplot(binary,aes(x=variable, y=value, fill=factor(group)))+geom_bar(stat='identity', position='dodge')+xlab('feature')+ylab('Probability of existing') + theme(text = element_text(size=10),
axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('probability per message')
df_avg$group = NULL
df_avg$email_length = total$email_length
email_grouped = df_avg %>% group_by(email_length) %>% summarise_all(funs(mean))
plot_num_word_vs_avg_feature = function(feature){
x = email_grouped$email_length
y = pull(email_grouped[feature])
df = data.frame(x,y)
ggplot(df, aes(x=x, y=y)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
}
plot_num_word_vs_avg_feature('Apology')
df = melt(df_grouped_bin[1,-1] - df_grouped_bin[3,-1])
ggplot(df, aes(x=variable, y=value)) + geom_point() + theme(text = element_text(size=10), axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('low - high') + geom_hline(yintercept = 0)
df = melt(df_grouped_bin[1,-1]/df_grouped_bin[3,-1])
ggplot(df, aes(x=variable, y=value)) + geom_point() + theme(text = element_text(size=10), axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('low / high') + geom_hline(yintercept = 1)
df_avg$email_length = NULL
df_low_avg = df_avg[1:10205,]
df_med_avg = df_avg[10206:21114,]
df_low_bin = df_bin[1:10205,]
df_med_bin = df_bin[10206:21114,]
df_low_avg$Apology = df_low_bin$Apology
df_med_avg$Apology = df_med_bin$Apology
df_low_grouped = df_low_avg %>% group_by(Apology) %>% summarise_all(funs(mean))
df_med_grouped = df_med_avg %>% group_by(Apology) %>% summarise_all(funs(mean))
low_avgs = melt(df_low_grouped, id.vars='Apology')
med_avgs = melt(df_med_grouped, id.vars='Apology')
ggplot(low_avgs,aes(x=variable, y=value, fill=factor(Apology)))+geom_bar(stat='identity', position='dodge')+xlab('feature')+ylab('Average per message') + theme(text = element_text(size=10),
axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('Low group - Average per message')
ggplot(med_avgs,aes(x=variable, y=value, fill=factor(Apology)))+geom_bar(stat='identity', position='dodge')+xlab('feature')+ylab('Average per message') + theme(text = element_text(size=10),
axis.text.x = element_text(angle=90, hjust=1, vjust=0.5)) + ggtitle('Med group - Average per message')
total = rbind(group_1, group_2, group_3)
df_count = politeness(total$`Wharton Export - Remove Delimiters`, metric='count')
df_avg = politeness(total$`Wharton Export - Remove Delimiters`, metric='average')
df_bin = politeness(total$`Wharton Export - Remove Delimiters`, metric='binary')
group = c(rep(1,10205), rep(2,10909), rep(3,17365))
df_count$group = group
df_avg$group = group
df_bin$group = group
View(df_avg)
View(email_grouped)
df_avg$email_length = total$email_length
df_avg_1 = df_avg$group == 1
df_avg_2 = df_avg$group == 2
df_avg_3 = df_avg$group == 3
email_grouped_1 = df_avg_1 %>% group_by(email_length) %>% summarise_all(funs(mean))
df_avg$email_length = total$email_length
df_avg_1 = df_avg[df_avg$group == 1,]
df_avg_2 = df_avg[df_avg$group == 2,]
df_avg_3 = df_avg[df_avg$group == 3,]
email_grouped_1 = df_avg_1 %>% group_by(email_length) %>% summarise_all(funs(mean))
email_grouped_2 = df_avg_2 %>% group_by(email_length) %>% summarise_all(funs(mean))
email_grouped_3 = df_avg_3 %>% group_by(email_length) %>% summarise_all(funs(mean))
plot_num_word_vs_avg_feature = function(grouped, feature){
x = grouped$email_length
y = pull(grouped[feature])
df = data.frame(x,y)
ggplot(df, aes(x=x, y=y)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
}
plot_num_word_vs_avg_feature(email_grouped_1, 'Apology')
plot_num_word_vs_avg_feature(email_grouped_2, 'Apology')
plot_num_word_vs_avg_feature(email_grouped_3, 'Apology')
plot_num_word_vs_avg_feature = function(feature){
x1 = email_grouped_1$email_length
x2 = email_grouped_2$email_length
x3 = email_grouped_3$email_length
y1 = pull(email_grouped_1[feature])
y2 = pull(email_grouped_2[feature])
y3 = pull(email_grouped_3[feature])
df1 = data.frame(x1,y1)
ggplot(df, aes(x=x1, y=y1)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
df2 = data.frame(x2,y2)
ggplot(df, aes(x=x2, y=y2)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
df2 = data.frame(x3,y3)
ggplot(df, aes(x=x3, y=y3)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
}
plot_num_word_vs_avg_feature('Apology')
plot_num_word_vs_avg_feature = function(grouped, feature){
x = grouped$email_length
y = pull(grouped[feature])
df = data.frame(x,y)
ggplot(df, aes(x=x, y=y)) + xlab('length') + ylab('Average per message') + geom_point(size=0.5) + geom_smooth(method='lm') + ggtitle(feature)
}
plot_num_word_vs_avg_feature(email_grouped_1, 'Apology')
plot_num_word_vs_avg_feature(email_grouped_2, 'Apology')
plot_num_word_vs_avg_feature(email_grouped_3, 'Apology')
plot_num_word_vs_avg_feature(email_grouped_1, 'Positive.Emotion')
plot_num_word_vs_avg_feature(email_grouped_2, 'Positive.Emotion')
plot_num_word_vs_avg_feature(email_grouped_2, 'Positive.Emotion')
plot_num_word_vs_avg_feature(email_grouped_3, 'Positive.Emotion')
plot_num_word_vs_avg_feature(email_grouped_1, 'Negative.Emotion')
plot_num_word_vs_avg_feature(email_grouped_2, 'Negative.Emotion')
plot_num_word_vs_avg_feature(email_grouped_3, 'Negative.Emotion')
plot_num_word_vs_avg_feature(email_grouped_1, 'First.Person.Plural')
plot_num_word_vs_avg_feature(email_grouped_2, 'First.Person.Plural')
plot_num_word_vs_avg_feature(email_grouped_2, 'First.Person.Plural')
plot_num_word_vs_avg_feature(email_grouped_3, 'First.Person.Plural')
plot_num_word_vs_avg_feature(email_grouped_1, 'First.Person.Single')
plot_num_word_vs_avg_feature(email_grouped_3, 'First.Person.Single')
plot_num_word_vs_avg_feature(email_grouped_1, 'Second.Person')
plot_num_word_vs_avg_feature(email_grouped_2, 'Second.Person')
plot_num_word_vs_avg_feature(email_grouped_3, 'Second.Person')
plot_num_word_vs_avg_feature(email_grouped_1, 'Gratitude')
plot_num_word_vs_avg_feature(email_grouped_2, 'Gratitude')
plot_num_word_vs_avg_feature(email_grouped_2, 'Gratitude')
plot_num_word_vs_avg_feature(email_grouped_3, 'Gratitude')
we_are_sorry_for_nf = glm(hotel$we_are_sorry_for_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(we_are_sorry_for_nf)$coef[2,]
exp(0.199532249)
we_are_sorry_for_nf = glm(hotel$we_are_sorry_for_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(we_are_sorry_for_nf)$coef[2,]
exp(summary(we_are_sorry_for_nf)$coef[2,])
we_are_sorry_for_nf = glm(hotel$we_are_sorry_for_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(we_are_sorry_for_nf)$coef[2,]
summary(we_are_sorry_for_nf)$coef
we_are_sorry_for_nf = glm(hotel$we_are_sorry_for_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(we_are_sorry_for_nf)$coef
we_are_sorry_for_nf = glm(hotel$we_are_sorry_for_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(we_are_sorry_for_nf)$coef
exp(summary(we_are_sorry_for_nf)$coef)
mcnemar(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
mcnemar.test(data, correct=TRUE)
data
}
mcnemar(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
data
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
hotel$i_apologize_for_n_bin
yes_yes = sum(hotel$i_apologize_for_n&hotel$i_apologize_that_n)
yes_yes
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar(hotel$we_apologize_for_n, hotel$we_apologize_that_n)
table(hotel$we_apologize_for_n_bin, hotel$we_apologize_that_n_bin)
mcnemar(hotel$we_apologize_that_n, hotel$we_are_sorry_that_n)
table(hotel$we_apologize_that_n_bin, hotel$we_are_sorry_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
table(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
table(hotel$i_am_sorry_for_n_bin, hotel$i_am_sorry_that_n_bin)
library(readr)
hotel = read_csv("hotel data/hotel_reg.csv")
group1 = hotel[hotel$group==1,]
group2 = hotel[hotel$group==2,]
group3 = hotel[hotel$group==3,]
i_apologize_that_nf = glm(hotel$i_apologize_that_n_bin ~ hotel$`Overall experience`, family=binomial)
summary(i_apologize_that_nf)$coef[2,]
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar(hotel$we_apologize_for_n, hotel$we_apologize_that_n)
table(hotel$we_apologize_for_n_bin, hotel$we_apologize_that_n_bin)
mcnemar(hotel$we_apologize_that_n, hotel$we_are_sorry_that_n)
table(hotel$we_apologize_that_n_bin, hotel$we_are_sorry_that_n_bin)
mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
table(hotel$i_am_sorry_for_n_bin, hotel$i_am_sorry_that_n_bin)
pronouns = read_csv("hotel data/pronouns.csv")
pronouns = read_csv("hotel data/pronoun_counts.csv")
View(pronouns)
153+33+343
74+97+444
pronouns = read_csv("hotel data/pronoun_counts.csv")
View(pronouns)
pronouns[pronouns$phrase == 'i_apolgoize_for_1']
pronouns[pronouns$phrase == 'i_apolgoize_for_1',]
pronouns[pronouns$phrase == 'i_apologize_for_1',]
pronouns[pronouns$phrase == 'i_apologize_for_2',]
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
mcnemar.test(data, correct=TRUE)
print(data)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(table(data))
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, yes_no, no_yes, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
mcnemar(hotel$we_apologize_for_n, hotel$we_apologize_that_n)
mcnemar(hotel$we_apologize_that_n, hotel$we_are_sorry_that_n)
mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
pronouns = read_csv("hotel data/pronoun_counts.csv")
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1',]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apolgoize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apolgoize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apolgoize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apolgoize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apolgoize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apolgoize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apolgoize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apolgoize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apolgoize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apolgoize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apolgoize_if_2',]
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
i_apologize_for_nf
pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apologize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apologize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apologize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apologize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apologize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apologize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apologize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apologize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apologize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apologize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apologize_if_2',]
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$he)
i_apologize_for_f
sum(i_apologize_for$we)
sum(i_apologize_for_f$we)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1$pronoun1)
n1 = sum(phrase1$total)
x2 = sum(phrase2$pronoun2)
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, we, we)
prop_test(i_apologize_for_f, i_apologize_for_nf, `we`, `we`)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(pronoun1)
n1 = sum(phrase1$total)
x2 = sum(pronoun2)
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
i_apologize_for[,'we']
i_apologize_for_f[,'we']
i_apologize_for_f[[,'we']]
i_apologize_for_f['we']
i_apologize_for_f[['we']]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase[[pronooun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
i_apologize_for_f[['we']]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronooun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$he)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
prop_test(i_apologize_for_nf, i_apologize_that_nf, 'we', 'we')
prop_test(we_apologize_that_nf, i_apologize_that_nf, 'we', 'we')
prop_test(i_apologize_if_nf, i_apologize_that_nf, 'we', 'we')
prop_test(i_apologize_if_nf, i_apologize_that_nf, 'you', 'you')
library(readr)
hotel = read_csv("hotel data/hotel_reg.csv")
group1 = hotel[hotel$group==1,]
group2 = hotel[hotel$group==2,]
group3 = hotel[hotel$group==3,]
pronouns = read_csv("hotel data/pronoun_counts.csv")
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apologize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apologize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apologize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apologize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apologize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apologize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apologize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apologize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apologize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apologize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apologize_if_2',]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_nf, i_apologize_that_nf, 'you', 'you')
install.packages('sf')
install.packages('raster')
install.packages('spData')
remotes::install_github('Nowosad/spDataLarge')
remotes::install_github('geocompr/geocompkg')
library(sf)
library(raster)
library(sf)
install.packages('sf')
install.packages('raster')
install.packages('spData')
remotes::install_github('Nowasad/spDataLarge')
install.packages('remotes')
remotes::install_github('Nowasad/spDataLarge')
remotes::install_github('Nowosad/spDataLarge')
remotes::install_github("geocompr/geocompkg")
Yes
remotes::install_github("geocompr/geocompkg")
library(sf)
library(raster)
library(spData)
library(spDataLarge)
names(world)
View(world)
world_asia = world[world$continent == "Asia",]
asia = st_union(world_asia)
plot(world['pop'], reset=FALSE)
plot(asia, add=TRUE, col='red')
world_asia[0]
world_asia[1]
a = matrix(1,1,2,3,nrow=2)
a = matrix(c(1,1,2,3),nrow=2)
a[0]
a[1]
a[2]
a[3]
a[4]
a[1,]
a[,1]
raster_filepath = system.file("raster/srtm.tif", package = "spDataLarge")
new_raster = raster(raster_filepath)
View(new_raster)
new_raster
plot(new_raster)
raster::writeFormats()
rgdal::gdalDrivers()
crs_data = rgdal::make_EPSG()
View(crs_data)
vector_filepath = system.file("vector/zion.gpkg", package = "spDataLarge")
new_vector = st_read(vector_filepath)
st_crs(new_vector)
library(dplyr)
library(stringr)
library(tidyr)
world %>%
summarize(pop = sum(pop, na.rm = TRUE), n = n())
install.packages('bookdown')
bookdown::render_book('/book/index.Rmd', 'bookdown::gitbook')
bookdown::render_book('~/book/index.Rmd', 'bookdown::gitbook')
setwd("~/Documents/micrometcalf/book")
bookdown::render_book('index.Rmd', 'bookdown::gitbook')
knitr::include_graphics('3-4.png')
# st_write(WV_county, "WV_county.shp")
# Set working directory
setwd("~/Desktop/WVCoal")
# Set working directory
setwd("Desktop/WVCoal")
# Set working directory
setwd("/Desktop/WVCoal")
setwd("/private/var/folders/y3/3bq1xjfd1q98y333xq7w7m7h0000gn/T/com.microsoft.Outlook/Outlook Temp")
setwd("~/Documents/micrometcalf/book")
install.packages('tidyverse')
install.packages('tidycensus')

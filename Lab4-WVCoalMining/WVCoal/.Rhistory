mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
table(hotel$i_am_sorry_for_n_bin, hotel$i_am_sorry_that_n_bin)
pronouns = read_csv("hotel data/pronouns.csv")
pronouns = read_csv("hotel data/pronoun_counts.csv")
View(pronouns)
153+33+343
74+97+444
pronouns = read_csv("hotel data/pronoun_counts.csv")
View(pronouns)
pronouns[pronouns$phrase == 'i_apolgoize_for_1']
pronouns[pronouns$phrase == 'i_apolgoize_for_1',]
pronouns[pronouns$phrase == 'i_apologize_for_1',]
pronouns[pronouns$phrase == 'i_apologize_for_2',]
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
mcnemar.test(data, correct=TRUE)
print(data)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(table(data))
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, no_yes, yes_no, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
table(hotel$i_apologize_for_n_bin, hotel$i_apologize_that_n_bin)
mcnemar = function(col1, col2){
yes_yes = sum(col1&col2)
yes_no = sum(col1) - yes_yes
no_yes = sum(col2) - yes_yes
no_no = sum(!col1&!col2)
data = matrix(c(yes_yes, yes_no, no_yes, no_no), nrow=2)
print(data)
mcnemar.test(data, correct=TRUE)
}
mcnemar(hotel$i_apologize_for_n, hotel$i_apologize_that_n)
mcnemar(hotel$we_apologize_for_n, hotel$we_apologize_that_n)
mcnemar(hotel$we_apologize_that_n, hotel$we_are_sorry_that_n)
mcnemar(hotel$i_am_sorry_for_n, hotel$i_am_sorry_that_n)
pronouns = read_csv("hotel data/pronoun_counts.csv")
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1',]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apolgoize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apolgoize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apolgoize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apolgoize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apolgoize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apolgoize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apolgoize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apolgoize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apolgoize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apolgoize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apolgoize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apolgoize_if_2',]
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
i_apologize_for_nf
pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apologize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apologize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apologize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apologize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apologize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apologize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apologize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apologize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apologize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apologize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apologize_if_2',]
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$we)
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$he)
i_apologize_for_f
sum(i_apologize_for$we)
sum(i_apologize_for_f$we)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1$pronoun1)
n1 = sum(phrase1$total)
x2 = sum(phrase2$pronoun2)
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, we, we)
prop_test(i_apologize_for_f, i_apologize_for_nf, `we`, `we`)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(pronoun1)
n1 = sum(phrase1$total)
x2 = sum(pronoun2)
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
i_apologize_for[,'we']
i_apologize_for_f[,'we']
i_apologize_for_f[[,'we']]
i_apologize_for_f['we']
i_apologize_for_f[['we']]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase[[pronooun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
prop_test(i_apologize_for_f, i_apologize_for_nf, i_apologize_for_f$we, i_apologize_for_nf$we)
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
i_apologize_for_f[['we']]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronooun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
mcnemar(i_apologize_for_nf$i, i_apologize_for_nf$he)
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_f, i_apologize_for_nf, 'we', 'we')
prop_test(i_apologize_for_nf, i_apologize_that_nf, 'we', 'we')
prop_test(we_apologize_that_nf, i_apologize_that_nf, 'we', 'we')
prop_test(i_apologize_if_nf, i_apologize_that_nf, 'we', 'we')
prop_test(i_apologize_if_nf, i_apologize_that_nf, 'you', 'you')
library(readr)
hotel = read_csv("hotel data/hotel_reg.csv")
group1 = hotel[hotel$group==1,]
group2 = hotel[hotel$group==2,]
group3 = hotel[hotel$group==3,]
pronouns = read_csv("hotel data/pronoun_counts.csv")
i_apologize_that_f = pronouns[pronouns$phrase == 'i_apologize_that_1',]
i_apologize_that_nf = pronouns[pronouns$phrase == 'i_apologize_that_2',]
i_apologize_for_f = pronouns[pronouns$phrase == 'i_apologize_for_1',]
i_apologize_for_nf = pronouns[pronouns$phrase == 'i_apologize_for_2',]
i_am_sorry_that_f = pronouns[pronouns$phrase == 'i_am_sorry_that_1',]
i_am_sorry_that_nf = pronouns[pronouns$phrase == 'i_am_sorry_that_2',]
i_am_sorry_for_f = pronouns[pronouns$phrase == 'i_am_sorry_for_1',]
i_am_sorry_for_nf = pronouns[pronouns$phrase == 'i_am_sorry_for_2',]
i_apologize_if_f = pronouns[pronouns$phrase == 'i_apologize_if_1',]
i_apologize_if_nf = pronouns[pronouns$phrase == 'i_apologize_if_2',]
we_apologize_that_f = pronouns[pronouns$phrase == 'we_apologize_that_1',]
we_apologize_that_nf = pronouns[pronouns$phrase == 'we_apologize_that_2',]
we_apologize_for_f = pronouns[pronouns$phrase == 'we_apologize_for_1',]
we_apologize_for_nf = pronouns[pronouns$phrase == 'we_apologize_for_2',]
we_are_sorry_that_f = pronouns[pronouns$phrase == 'we_are_sorry_that_1',]
we_are_sorry_that_nf = pronouns[pronouns$phrase == 'we_are_sorry_that_2',]
we_are_sorry_for_f = pronouns[pronouns$phrase == 'we_are_sorry_for_1',]
we_are_sorry_for_nf = pronouns[pronouns$phrase == 'we_are_sorry_for_2',]
we_apologize_if_f = pronouns[pronouns$phrase == 'we_apologize_if_1',]
we_apologize_if_nf = pronouns[pronouns$phrase == 'we_apologize_if_2',]
prop_test = function(phrase1, phrase2, pronoun1, pronoun2){
x1 = sum(phrase1[[pronoun1]])
n1 = sum(phrase1$total)
x2 = sum(phrase2[[pronoun2]])
n2 = sum(phrase2$total)
result = prop.test(x=c(x1, x2), n=c(n1, n2))
print(result)
}
prop_test(i_apologize_for_nf, i_apologize_that_nf, 'you', 'you')
install.packages('sf')
install.packages('raster')
install.packages('spData')
remotes::install_github('Nowosad/spDataLarge')
remotes::install_github('geocompr/geocompkg')
library(sf)
library(raster)
library(sf)
install.packages('sf')
install.packages('raster')
install.packages('spData')
remotes::install_github('Nowasad/spDataLarge')
install.packages('remotes')
remotes::install_github('Nowasad/spDataLarge')
remotes::install_github('Nowosad/spDataLarge')
remotes::install_github("geocompr/geocompkg")
Yes
remotes::install_github("geocompr/geocompkg")
library(sf)
library(raster)
library(spData)
library(spDataLarge)
names(world)
View(world)
world_asia = world[world$continent == "Asia",]
asia = st_union(world_asia)
plot(world['pop'], reset=FALSE)
plot(asia, add=TRUE, col='red')
world_asia[0]
world_asia[1]
a = matrix(1,1,2,3,nrow=2)
a = matrix(c(1,1,2,3),nrow=2)
a[0]
a[1]
a[2]
a[3]
a[4]
a[1,]
a[,1]
raster_filepath = system.file("raster/srtm.tif", package = "spDataLarge")
new_raster = raster(raster_filepath)
View(new_raster)
new_raster
plot(new_raster)
raster::writeFormats()
rgdal::gdalDrivers()
crs_data = rgdal::make_EPSG()
View(crs_data)
vector_filepath = system.file("vector/zion.gpkg", package = "spDataLarge")
new_vector = st_read(vector_filepath)
st_crs(new_vector)
library(dplyr)
library(stringr)
library(tidyr)
world %>%
summarize(pop = sum(pop, na.rm = TRUE), n = n())
install.packages('bookdown')
bookdown::render_book('/book/index.Rmd', 'bookdown::gitbook')
bookdown::render_book('~/book/index.Rmd', 'bookdown::gitbook')
setwd("~/Documents/micrometcalf/book")
bookdown::render_book('index.Rmd', 'bookdown::gitbook')
knitr::include_graphics('3-4.png')
# st_write(WV_county, "WV_county.shp")
# Set working directory
setwd("~/Desktop/WVCoal")
# Set working directory
setwd("Desktop/WVCoal")
# Set working directory
setwd("/Desktop/WVCoal")
setwd("/private/var/folders/y3/3bq1xjfd1q98y333xq7w7m7h0000gn/T/com.microsoft.Outlook/Outlook Temp")
setwd("~/Documents/micrometcalf/book")
install.packages('tidyverse')
install.packages('tidycensus')
library(bookdown)
setwd("~/Documents/micrometcalf/Intro2GIS/book")
bookdown::render_book('index.Rmd')
# Set working directory
setwd("/Users/maryniakolak/Desktop/WVCoal")
# Set working directory
#setwd("/Users/maryniakolak/Desktop/WVCoal")
setwd("~/Documents/micrometcalf/Intro2GIS/book")
# install.packages("sf")
library(sf)
library(tidyverse)
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
# Load the output area shapefiles
County.Areas<- st_read("WV_Counties.shp")
setwd("~/Documents/micrometcalf/Intro2GIS/book/Lab4-WVCoalMining")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
setwd("~/Documents/micrometcalf/Intro2GIS/book/Lab4-WVCoalMining/WVCoal")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
# Load new census data you plan to join.
Census.Data <-read.csv("/ACS_15_5YR_DP03.csv")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
read.csv("ACS_15_5YR_DP03.csv")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
# Load new census data you plan to join.
Census.Data <-read.csv("ACS_15_5YR_DP03.csv")
# Load the output area shapefiles
County.Areas<- st_read("WV_Counties.shp")
# View the attributes associated with each county
glimpse(County.Areas)
# View the attributes associated with each county
# head(Census.Data)
# View the attributes associated with each county
dim(Census.Data)
#str(Census.Data)
var <- c("GEO.id2","HC01_VC85","HC03_VC50")
Census.Subset<- (Census.Data[var])
glimpse(Census.Subset)
names(Census.Subset)[2:3]<- c("MedInc15", "AgMnJb15")
glimpse(Census.Subset)
# Joins data to the shapefile
WV_county <- merge(County.Areas, Census.Subset, by.x="FIPSSTCO", by.y="GEO.id2")
head(WV_county)
library(tmap)
library(RColorBrewer)
# Display the color palette
display.brewer.all()
# Creates a simple choropleth map of our a AgMnJb15 variable
tm_shape(WV_county) + tm_fill("AgMnJb15")
# setting a color palette
tm_shape(WV_county) + tm_fill("AgMnJb15", palette = "-BrBG")
# changing the intervals
tm_shape(WV_county) + tm_fill("AgMnJb15", style = "quantile", palette = "-BrBG")
# number of levels
tm_shape(WV_county) + tm_fill("AgMnJb15", style = "quantile", n = 7, palette = "-BrBG")
# includes a histogram in the legend
tm_shape(WV_county) + tm_fill("AgMnJb15", style = "jenks", n=6, palette = "-BrBG", legend.hist = TRUE)
# add borders
tm_shape(WV_county) + tm_fill("AgMnJb15", style = "jenks", n=6, palette = "-BrBG") +
tm_borders(alpha=.4)
# north arrow
tm_shape(WV_county) + tm_fill("AgMnJb15", style = "jenks", n=6, palette = "-BrBG") +
tm_borders(alpha=.4) +
tm_compass()
# adds in layout, gets rid of frame
tm_shape(WV_county) + tm_fill("AgMnJb15", palette = "-BrBG", style = "jenks", title = "% Jobs High Risk Employment") +
tm_borders(alpha=.4) +
tm_compass() +
tm_layout(legend.text.size = 1.1, legend.title.size = 1.5, legend.position = c("right", "bottom"), frame = FALSE)
# ReMap
BLSData <- tm_shape(WV_county) + tm_fill("AveEmp15", palette = "BuPu", style = "jenks", title = "Coal Mng Jobs") +
tm_borders(alpha=.4) +
tm_layout(legend.text.size = .8, legend.title.size = 1.0, legend.position = c("right", "bottom"), frame = FALSE)
ACSData <- tm_shape(WV_county) + tm_fill("AgMnJb15", palette = "BuPu", style = "jenks", title = "High Risk Job %") +
tm_borders(alpha=.4) +
tm_layout(legend.text.size = .8, legend.title.size = 1.0, legend.position = c("right", "bottom"), frame = FALSE)
tmap_arrange(BLSData,ACSData)
library(tidyverse)
library(tidycensus)
options(tigris_use_cache = TRUE)
census_api_key("408ee78177f8c46c2d2089befaa8c24a55549773", install=TRUE, overwrite = TRUE)
ACS15var <- load_variables(2015, "acs5", cache = TRUE)
#view(ACS15var)
WV_county_medinc <- get_acs(geography = "county",
variables = c(medincome = "B19013_001"),
state = "WV",
year = 2015)
WV_county_medinc <- get_acs(geography = "county",
variables = c(medincome = "B19013_001"),
state = "WV",
year = 2015)
#census_api_key("YOURAPIKEY", install=TRUE, overwrite = TRUE)
census_api_key("408ee78177f8c46c2d2089befaa8c24a55549773", install=TRUE, overwrite = TRUE)
ACS15var <- load_variables(2015, "acs5", cache = TRUE)
#view(ACS15var)
WV_county_medinc <- get_acs(geography = "county",
variables = c(medincome = "B19013_001"),
state = "WV",
year = 2015)
#census_api_key("YOURAPIKEY", install=TRUE, overwrite = TRUE)
readRenviron("~/.Renviron")
WV_county_medinc <- get_acs(geography = "county",
variables = c(medincome = "B19013_001"),
state = "WV",
year = 2015)
WV_county_medinc
WV_county_medinc.sp <- get_acs(state = "WV",
geography = "county",
variables = "B19013_001",
geometry = TRUE)
head(WV_county_medinc.sp)
WV_county_medinc.sp %>%
ggplot(aes(fill = estimate)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
tm_shape(WV_county_medinc.sp) + tm_fill("estimate", palette = "BuPu", title ="Median Income")
tm_shape(WV_county_medinc.sp) + tm_fill("estimate", palette = "BuPu", title ="Median Income")
WV_tracts_medinc.sp <- get_acs(state = "WV",
geography = "tract",
variables = "B19013_001",
geometry = TRUE)
head(WV_tracts_medinc.sp)
WV_tracts_medinc.sp %>%
ggplot(aes(fill = estimate)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
tm_shape(WV_tracts_medinc.sp) + tm_fill("estimate", style = "jenks", palette = "BuPu", title ="Median Income")
WV_county_mining.sp <- get_acs(state = "WV",
geography = "county",
variables = "B24031_002",
geometry = TRUE)
WV_tracts_mining.sp <- get_acs(state = "WV",
geography = "tract",
variables = "B24031_002",
geometry = TRUE)
# ReMap
WVCounties <- tm_shape(WV_county_mining.sp) + tm_fill("estimate", n=4, palette = "BuPu", style = "jenks", title = "High Risk Jobs") +
tm_borders(alpha=.4) +
tm_layout(legend.text.size = .8, legend.title.size = 1.0, legend.position = c("right", "bottom"), frame = FALSE)
WVTracts <- tm_shape(WV_tracts_mining.sp) + tm_fill("estimate", n=4, palette = "BuPu", style = "jenks", title = "High Risk Jobs") +
tm_borders(alpha=.4) +
tm_layout(legend.text.size = .8, legend.title.size = 1.0, legend.position = c("right", "bottom"), frame = FALSE)
tmap_arrange(WVCounties,WVTracts)
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(tmap)
library(leaflet)
library(data.table)
library(tidyverse)
#setwd("~/Desktop/Lab5-LACrimes")
LAcrime<-fread("LAPD2015_Violent.csv", header = T)
head(LAcrime)
unique(LAcrime$`Crime Code Description`)
LAcrime.df<- as.data.frame(LAcrime)
# Base R subset:
s1<-subset(LAcrime.df,LAcrime.df$`Crime Code Description`== "CRIMINAL HOMICIDE")
LAcrime.hom<-rbind(s1)
head(s1)
write.csv(LAcrime.hom,"LAcrime_hom.csv")
glimpse(LAcrime.hom[,c("longitude","latitude")])
str(LAcrime.hom[,c("longitude", "latitude")])
LAcrime.hom$latitude <- as.numeric(LAcrime.hom$latitude)
LAcrime.hom$longitude <- as.numeric(LAcrime.hom$longitude)
LAcrime.pts <- st_as_sf(LAcrime.hom, coords = c("longitude","latitude"), crs = 4326)
LAcrime.hom.na <- subset(LAcrime.hom, is.na(LAcrime.hom[,c("longitude", "latitude")]))
glimpse(LAcrime.hom.na) #1 observations
LAcrime.hom2 <- na.omit(LAcrime.hom[,c("DR Number","longitude", "latitude")])
str(LAcrime.hom2)
LAcrime.pts <- st_as_sf(LAcrime.hom2, coords = c("longitude","latitude"), crs = 4326)
plot(LAcrime.pts)
LAcrime.pts <- st_as_sf(LAcrime.hom2, coords = c("longitude","latitude"), crs = 4326)
plot(LAcrime.pts)
#st_write(LAcrime.pts,"LAcrime_hom.shp")
LAcrimes<-LAcrime.pts
## 1st layer (gets plotted first)
tm_shape(LAtracts) + tm_borders(alpha = 0.4) +
## 2nd layer (overlay)
tm_shape(LAcrime.pts) + tm_dots(size = 0.1, col="red")
LAtracts <- st_read("LAC_Shape.shp")
## 1st layer (gets plotted first)
tm_shape(LAtracts) + tm_borders(alpha = 0.4) +
## 2nd layer (overlay)
tm_shape(LAcrime.pts) + tm_dots(size = 0.1, col="red")
st_crs(LAcrimes)
st_crs(LAtracts)
CRS.new <- st_crs(LAtracts)
LAcrimes <- st_transform(LAcrimes, CRS.new)
glimpse(LAcrimes)
crime_in_tract <- st_join(LAcrimes, LAtracts, join = st_within)
glimpse(crime_in_tract)
crime_tract_count <- as.data.frame(table(crime_in_tract$TRACTCE10))
glimpse(crime_tract_count)
names(crime_tract_count) <- c("TRACTCE10","CrimeCt")
glimpse(crime_tract_count)
LAtracts_new <- merge(LAtracts, crime_tract_count, by="TRACTCE10")
glimpse(LAtracts_new)
tm_shape(LAtracts_new) + tm_fill("CrimeCt", n=4, pal = "BuPu", title="LA Homicides in 2015")
tmap_mode("view")
tm_shape(LAtracts_new) + tm_fill("CrimeCt", n=4, pal = "BuPu", title="LA Homicides in 2015")
